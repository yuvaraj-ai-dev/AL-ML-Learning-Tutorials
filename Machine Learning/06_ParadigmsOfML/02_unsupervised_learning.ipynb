{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff80de3",
   "metadata": {},
   "source": [
    "# Module 2 — Unsupervised Learning\n",
    "\n",
    "**Created:** 2025-12-04 14:06:54 UTC\n",
    "\n",
    "## Overview\n",
    "\n",
    "Unsupervised learning finds structure in unlabeled data: clustering, dimensionality reduction, density estimation.\n",
    "\n",
    "### Why Unsupervised Learning?\n",
    "\n",
    "Unsupervised learning is particularly useful when labeled data is scarce or expensive to obtain. In many real-world scenarios, collecting labels for data can be time-consuming, costly, or even impossible (e.g., for historical data or new domains). Instead of relying on supervised methods that require labeled examples, unsupervised algorithms discover hidden patterns, group similar data points, or reduce data complexity without explicit guidance.\n",
    "\n",
    "### Where is it Applied?\n",
    "\n",
    "- **Clustering Customer Segments**: Grouping customers based on purchasing behavior to personalize marketing strategies.\n",
    "- **Anomaly Detection**: Identifying unusual patterns in network traffic for cybersecurity or detecting fraud in financial transactions.\n",
    "- **Image Segmentation**: Partitioning images into meaningful regions for computer vision tasks.\n",
    "- **Topic Modeling**: Discovering themes in large text corpora for content recommendation.\n",
    "- **Recommendation Systems**: Finding similar items or users without explicit ratings.\n",
    "\n",
    "### When to Choose Unsupervised Learning?\n",
    "\n",
    "- **Data Exploration**: When you want to understand the underlying structure of your data without predefined categories.\n",
    "- **Pattern Discovery**: For finding hidden relationships or groupings that aren't obvious.\n",
    "- **Dimensionality Reduction**: To simplify high-dimensional data for visualization or faster processing.\n",
    "- **Pre-processing**: As a first step before applying supervised learning, to reduce noise or extract features.\n",
    "\n",
    "## Learning objectives\n",
    "- Understand clustering vs dimensionality reduction.\n",
    "- Run KMeans on synthetic data (beginner).\n",
    "- Try PCA and t-SNE (intermediate).\n",
    "- Advanced: mixture models, clustering validation, and practical tips.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00f676",
   "metadata": {},
   "source": [
    "## Beginner — Concept + Simple Example\n",
    "\n",
    "### What is Clustering?\n",
    "\n",
    "Clustering is like organizing a messy room: you group similar items together without knowing exactly what each group should be called. Imagine you have a bunch of different fruits scattered around - apples with apples, oranges with oranges, etc. Clustering algorithms do this automatically by finding natural groupings based on similarities.\n",
    "\n",
    "Another analogy: Think of clustering as sorting a pile of mixed coins into piles of the same denomination. You don't know the labels (\"nickel,\" \"dime\"), but you can group them based on size, weight, and appearance.\n",
    "\n",
    "**Key Concept:** No labels — algorithms try to group or summarize data based on patterns they discover.\n",
    "\n",
    "### K-Means Clustering: The Basics\n",
    "\n",
    "K-Means is the most popular clustering algorithm. Think of it as placing 'k' shop centers in a city so that each house is closest to its nearest center. The algorithm:\n",
    "1. Randomly places k 'centroids' (centers)\n",
    "2. Assigns each data point to the nearest centroid\n",
    "3. Moves centroids to the average position of their assigned points\n",
    "4. Repeats until centroids stop moving\n",
    "\n",
    "**Simple Example:** KMeans clustering on synthetic data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45abe7ea",
   "metadata": {},
   "outputs": [],
   "source": [
   "# Beginner example\n",
   "from sklearn.datasets import make_blobs\n",
   "from sklearn.cluster import KMeans\n",
   "\n",
   "X, y_true = make_blobs(n_samples=300, centers=4, random_state=42)\n",
   "kmeans = KMeans(n_clusters=4, random_state=42)\n",
   "kmeans.fit(X)\n",
   "print('Cluster centers:\\n', kmeans.cluster_centers_)\n",
   "print('Inertia (lower is better):', kmeans.inertia_)\n"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "hierarchical_md",
  "metadata": {},
  "source": [
   "### Hierarchical Clustering: Building a Tree of Clusters\n",
   "\n",
   "Hierarchical clustering is like creating a family tree of your data. It starts by treating each data point as its own cluster, then merges the closest clusters step by step until one big cluster remains. Analogy: Like building a tree where leaves are individual items, and branches represent groups merging together.\n",
   "\n",
   "**Key Concept:** Produces a dendrogram (tree diagram) showing the hierarchy of clusters.\n",
   "\n",
   "**Example:** Hierarchical clustering on the same synthetic data.\n"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "id": "hierarchical_code",
  "metadata": {},
  "outputs": [],
  "source": [
   "# Import necessary libraries\n",
   "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
   "import matplotlib.pyplot as plt\n",
   "\n",
   "# Generate the same synthetic data\n",
   "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
   "\n",
   "# Perform hierarchical clustering using Ward's method (minimizes variance)\n",
   "linked = linkage(X, method='ward')\n",
   "\n",
   "# Plot the dendrogram\n",
   "plt.figure(figsize=(10, 7))\n",
   "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
   "plt.title('Hierarchical Clustering Dendrogram')\n",
   "plt.show()\n",
   "\n",
   "# Cut the dendrogram to get 4 clusters\n",
   "clusters = fcluster(linked, 4, criterion='maxclust')\n",
   "print('Number of clusters found:', len(set(clusters)))\n"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "dbscan_md",
  "metadata": {},
  "source": [
   "### DBSCAN: Density-Based Clustering\n",
   "\n",
   "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is like finding islands in the ocean. It groups dense regions of points together, leaving sparse areas as noise. Analogy: Imagine points as people at a party; dense groups are clusters, isolated people are outliers.\n",
   "\n",
   "**Key Concept:** Doesn't require specifying the number of clusters beforehand. Good for arbitrary-shaped clusters and handling noise.\n",
   "\n",
   "**Example:** DBSCAN on synthetic data with noise.\n"
  ]
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "id": "dbscan_code",
  "metadata": {},
  "outputs": [],
  "source": [
   "# Import necessary libraries\n",
   "from sklearn.cluster import DBSCAN\n",
   "from sklearn.datasets import make_blobs\n",
   "import numpy as np\n",
   "import matplotlib.pyplot as plt\n",
   "\n",
   "# Generate data with some noise\n",
   "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
   "\n",
   "# Add some noise points\n",
   "X_noise = np.random.uniform(low=-10, high=10, size=(50, 2))\n",
   "X_with_noise = np.vstack([X, X_noise])\n",
   "\n",
   "# Apply DBSCAN\n",
   "dbscan = DBSCAN(eps=0.5, min_samples=5)  # eps: neighborhood distance, min_samples: core point threshold\n",
   "clusters = dbscan.fit_predict(X_with_noise)\n",
   "\n",
   "# Visualize results (noise points are labeled -1)\n",
   "plt.scatter(X_with_noise[:, 0], X_with_noise[:, 1], c=clusters, cmap='plasma')\n",
   "plt.title('DBSCAN Clustering (Noise points in gray)')\n",
   "plt.show()\n",
   "\n",
   "print('Number of clusters found (excluding noise):', len(set(clusters)) - (1 if -1 in clusters else 0))\n"
  ]
 },
 {
  "cell_type": "markdown",
  "id": "7a4b880f",
  "metadata": {},
  "source": [
   "## Intermediate — Dimensionality reduction & visualization\n",
    "\n",
    "**What to learn:** PCA for compression, t-SNE/UMAP for visualization, interpreting components.\n",
    "\n",
    "**Code idea:** Apply PCA then plot first two principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate example\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2 = pca.fit_transform(X)\n",
    "plt.scatter(X2[:,0], X2[:,1])\n",
    "plt.title('PCA projection (2D)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6597f",
   "metadata": {},
   "source": [
    "## Advanced — Probabilistic models & cluster validation\n",
    "\n",
    "**Topics:** Gaussian Mixture Models, Silhouette score, Davies–Bouldin, stability, choosing k.\n",
    "\n",
    "**Advanced code sketch:** Fit a GMM and compute silhouette score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc30715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced example (sketch)\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "gmm = GaussianMixture(n_components=4, random_state=42)\n",
    "gmm.fit(X)\n",
    "labels = gmm.predict(X)\n",
    "print('Silhouette score:', silhouette_score(X, labels))\n",
    "\n",
    "# Tips:\n",
    "# - Try multiple initializations\n",
    "# - Scale features before distance-based clustering\n"
   ]
  }
 ],
 "metadata": {
  "title": "Module 2 — Unsupervised Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
