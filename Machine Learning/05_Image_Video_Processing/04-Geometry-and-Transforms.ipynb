{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometry and Transforms in Image Processing\n",
    "\n",
    "Welcome to this comprehensive tutorial on geometric transformations in image processing! Geometric transforms allow us to manipulate the spatial arrangement of pixels in an image. This is fundamental for tasks like image alignment, object detection, augmentation, and many computer vision applications.\n",
    "\n",
    "**What we'll cover:**\n",
    "- **Resizing**: Changing image dimensions\n",
    "- **Rotation**: Rotating images around a point\n",
    "- **Affine Transforms**: Preserving parallel lines (translation, rotation, scaling, shearing)\n",
    "- **Perspective Transforms**: Correcting for 3D perspective (warping)\n",
    "- **Image Pyramids**: Multi-scale representations\n",
    "\n",
    "Each section builds progressively - we'll start with simple 2D operations and move to more complex transformations that account for 3D geometry.\n",
    "\n",
    "**Prerequisites:** Basic understanding of NumPy arrays, OpenCV, and matplotlib.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure matplotlib for better display\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Resizing\n",
    "\n",
    "**What is resizing?**\n",
    "Resizing changes the spatial resolution of an image by either shrinking or enlarging it. This is one of the most fundamental geometric operations.\n",
    "\n",
    "**When to use:**\n",
    "- Preparing images for neural network input (standardizing sizes)\n",
    "- Reducing storage space\n",
    "- Improving computational efficiency\n",
    "- Creating thumbnails\n",
    "\n",
    "**Key concepts:**\n",
    "- **Interpolation**: Method used to estimate pixel values when enlarging\n",
    "- **Aspect ratio**: Width/height relationship to maintain\n",
    "- **Scaling factors**: How much to scale in x and y directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image for demonstration\n",
    "img = cv2.imread('datasets/sample_images/sample1.png')\n",
    "if img is None:\n",
    "    print(\"Error: Could not load image. Please check the path.\")\n",
    "else:\n",
    "    print(f\"Original image shape: {img.shape}\")\n",
    "    \n",
    "# Convert BGR to RGB for matplotlib\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing Examples\n",
    "\n",
    "Let's explore different resizing techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic resizing to specific dimensions\n",
    "height, width = img.shape[:2]\n",
    "new_width, new_height = 300, 200\n",
    "\n",
    "# Using cv2.resize with default interpolation (bilinear)\n",
    "resized_basic = cv2.resize(img, (new_width, new_height))\n",
    "resized_basic_rgb = cv2.cvtColor(resized_basic, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f\"Resized to: {resized_basic.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Scaling by factors\n",
    "scale_x, scale_y = 0.5, 0.5  # Scale to half size\n",
    "resized_scaled = cv2.resize(img, None, fx=scale_x, fy=scale_y)\n",
    "resized_scaled_rgb = cv2.cvtColor(resized_scaled, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f\"Scaled image shape: {resized_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Different interpolation methods\n",
    "# For shrinking (reducing size), use INTER_AREA\n",
    "resized_area = cv2.resize(img, (200, 150), interpolation=cv2.INTER_AREA)\n",
    "resized_area_rgb = cv2.cvtColor(resized_area, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# For enlarging, use INTER_CUBIC or INTER_LINEAR\n",
    "resized_cubic = cv2.resize(img, (600, 450), interpolation=cv2.INTER_CUBIC)\n",
    "resized_cubic_rgb = cv2.cvtColor(resized_cubic, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f\"Area interpolation shape: {resized_area.shape}\")\n",
    "print(f\"Cubic interpolation shape: {resized_cubic.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of resizing methods\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original\n",
    "axes[0,0].imshow(img_rgb)\n",
    "axes[0,0].set_title('Original Image')\n",
    "axes[0,0].axis('off')\n",
    "\n",
    "# Basic resize\n",
    "axes[0,1].imshow(resized_basic_rgb)\n",
    "axes[0,1].set_title('Basic Resize (300x200)')\n",
    "axes[0,1].axis('off')\n",
    "\n",
    "# Scaled\n",
    "axes[0,2].imshow(resized_scaled_rgb)\n",
    "axes[0,2].set_title('Scaled (0.5x)')\n",
    "axes[0,2].axis('off')\n",
    "\n",
    "# Area interpolation\n",
    "axes[1,0].imshow(resized_area_rgb)\n",
    "axes[1,0].set_title('Area Interp. (200x150)')\n",
    "axes[1,0].axis('off')\n",
    "\n",
    "# Cubic interpolation\n",
    "axes[1,1].imshow(resized_cubic_rgb)\n",
    "axes[1,1].set_title('Cubic Interp. (600x450)')\n",
    "axes[1,1].axis('off')\n",
    "\n",
    "# Zoom on cubic for detail\n",
    "axes[1,2].imshow(resized_cubic_rgb[100:200, 200:300])\n",
    "axes[1,2].set_title('Zoomed Detail (Cubic)')\n",
    "axes[1,2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips for Resizing:**\n",
    "- Use `cv2.INTER_AREA` for shrinking (better quality)\n",
    "- Use `cv2.INTER_CUBIC` for enlarging (slower but higher quality)\n",
    "- `cv2.INTER_LINEAR` is a good default for most cases\n",
    "- Always check aspect ratio preservation if needed\n",
    "- Consider memory implications when enlarging images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Rotation\n",
    "\n",
    "**What is rotation?**\n",
    "Rotation transforms an image by turning it around a center point by a specified angle.\n",
    "\n",
    "**Key concepts:**\n",
    "- **Center of rotation**: Usually image center, but can be any point\n",
    "- **Rotation matrix**: Mathematical transformation matrix\n",
    "- **Bounding box**: New image dimensions after rotation\n",
    "\n",
    "**Applications:**\n",
    "- Correcting skewed documents\n",
    "- Data augmentation for machine learning\n",
    "- Aligning images for stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic rotation around center\n",
    "height, width = img.shape[:2]\n",
    "center = (width // 2, height // 2)\n",
    "angle = 45  # degrees\n",
    "\n",
    "# Get rotation matrix\n",
    "rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "# Apply rotation\n",
    "rotated = cv2.warpAffine(img, rotation_matrix, (width, height))\n",
    "rotated_rgb = cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f\"Rotation matrix:\\n{rotation_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Rotation with different centers and scaling\n",
    "# Rotate around top-left corner\n",
    "center_tl = (0, 0)\n",
    "rotation_matrix_tl = cv2.getRotationMatrix2D(center_tl, 30, 1.0)\n",
    "rotated_tl = cv2.warpAffine(img, rotation_matrix_tl, (width, height))\n",
    "rotated_tl_rgb = cv2.cvtColor(rotated_tl, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Rotate with scaling\n",
    "rotation_matrix_scaled = cv2.getRotationMatrix2D(center, -60, 0.7)\n",
    "rotated_scaled = cv2.warpAffine(img, rotation_matrix_scaled, (width, height))\n",
    "rotated_scaled_rgb = cv2.cvtColor(rotated_scaled, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Automatic bounding box calculation\n",
    "# Calculate new dimensions to fit entire rotated image\n",
    "cos = np.abs(rotation_matrix[0, 0])\n",
    "sin = np.abs(rotation_matrix[0, 1])\n",
    "\n",
    "new_width_calc = int((height * sin) + (width * cos))\n",
    "new_height_calc = int((height * cos) + (width * sin))\n",
    "\n",
    "# Adjust rotation matrix for new center\n",
    "rotation_matrix[0, 2] += (new_width_calc / 2) - center[0]\n",
    "rotation_matrix[1, 2] += (new_height_calc / 2) - center[1]\n",
    "\n",
    "rotated_fitted = cv2.warpAffine(img, rotation_matrix, (new_width_calc, new_height_calc))\n",
    "rotated_fitted_rgb = cv2.cvtColor(rotated_fitted, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f\"Fitted dimensions: {rotated_fitted.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of rotations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axes[0,0].imshow(img_rgb)\n",
    "axes[0,0].set_title('Original')\n",
    "axes[0,0].axis('off')\n",
    "\n",
    "axes[0,1].imshow(rotated_rgb)\n",
    "axes[0,1].set_title('Rotated 45° (center)')\n",
    "axes[0,1].axis('off')\n",
    "\n",
    "axes[0,2].imshow(rotated_tl_rgb)\n",
    "axes[0,2].set_title('Rotated 30° (corner)')\n",
    "axes[0,2].axis('off')\n",
    "\n",
    "axes[1,0].imshow(rotated_scaled_rgb)\n",
    "axes[1,0].set_title('Rotated -60° + Scale 0.7')\n",
    "axes[1,0].axis('off')\n",
    "\n",
    "axes[1,1].imshow(rotated_fitted_rgb)\n",
    "axes[1,1].set_title('Fitted Rotation')\n",
    "axes[1,1].axis('off')\n",
    "\n",
    "# Show rotation matrix effect\n",
    "axes[1,2].text(0.1, 0.5, f'Rotation Matrix:\\n{rotation_matrix}', \n",
    "               transform=axes[1,2].transAxes, fontsize=10, verticalalignment='center')\n",
    "axes[1,2].set_title('Matrix Details')\n",
    "axes[1,2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rotation Tips:**\n",
    "- Negative angles rotate clockwise\n",
    "- Use `cv2.warpAffine` for rotation\n",
    "- Calculate bounding box to avoid cropping\n",
    "- Rotation is an affine transform (preserves parallel lines)\n",
    "- Consider interpolation quality for smooth results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Affine Transforms\n",
    "\n",
    "**What are affine transforms?**\n",
    "Affine transformations preserve parallel lines and ratios of distances. They include:\n",
    "- Translation (shifting)\n",
    "- Rotation\n",
    "- Scaling\n",
    "- Shearing (skewing)\n",
    "- Combinations of above\n",
    "\n",
    "**Mathematical representation:**\n",
    "Any affine transform can be represented by a 2x3 matrix:\n",
    "```\n",
    "[a11 a12 b1]\n",
    "[a21 a22 b2]\n",
    "```\n",
    "\n",
    "**Applications:**\n",
    "- Image registration and alignment\n",
    "- Perspective correction for small angles\n",
    "- Data augmentation\n",
    "- Geometric normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Translation (shifting)\n",
    "tx, ty = 100, 50  # Shift right by 100, down by 50\n",
    "translation_matrix = np.float32([[1, 0, tx],\n",
    "                                [0, 1, ty]])\n",
    "\n",
    "translated = cv2.warpAffine(img, translation_matrix, (width + tx, height + ty))\n",
    "translated_rgb = cv2.cvtColor(translated, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f\"Translation matrix:\\n{translation_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Shearing (skewing)\n",
    "# Horizontal shear\n",
    "shear_x = 0.3\n",
    "shear_matrix_x = np.float32([[1, shear_x, 0],\n",
    "                            [0, 1, 0]])\n",
    "\n",
    "sheared_x = cv2.warpAffine(img, shear_matrix_x, (int(width + height*shear_x), height))\n",
    "sheared_x_rgb = cv2.cvtColor(sheared_x, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Vertical shear\n",
    "shear_y = 0.2\n",
    "shear_matrix_y = np.float32([[1, 0, 0],\n",
    "                            [shear_y, 1, 0]])\n",
    "\n",
    "sheared_y = cv2.warpAffine(img, shear_matrix_y, (width, int(height + width*shear_y)))\n",
    "sheared_y_rgb = cv2.cvtColor(sheared_y, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Complex affine transform (rotation + translation + scaling)\n",
    "# Define three points in source image\n",
    "pts1 = np.float32([[50, 50], [200, 50], [50, 200]])\n",
    "\n",
    "# Define where these points should map to in destination\n",
    "pts2 = np.float32([[100, 100], [250, 80], [80, 250]])\n",
    "\n",
    "# Get affine transform matrix\n",
    "affine_matrix = cv2.getAffineTransform(pts1, pts2)\n",
    "\n",
    "# Apply transform\n",
    "affine_transformed = cv2.warpAffine(img, affine_matrix, (width, height))\n",
    "affine_transformed_rgb = cv2.cvtColor(affine_transformed, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f\"Affine matrix:\\n{affine_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of affine transforms\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axes[0,0].imshow(img_rgb)\n",
    "axes[0,0].set_title('Original')\n",
    "axes[0,0].axis('off')\n",
    "\n",
    "axes[0,1].imshow(translated_rgb)\n",
    "axes[0,1].set_title('Translated (+100, +50)')\n",
    "axes[0,1].axis('off')\n",
    "\n",
    "axes[0,2].imshow(sheared_x_rgb)\n",
    "axes[0,2].set_title('Horizontal Shear (0.3)')\n",
    "axes[0,2].axis('off')\n",
    "\n",
    "axes[1,0].imshow(sheared_y_rgb)\n",
    "axes[1,0].set_title('Vertical Shear (0.2)')\n",
    "axes[1,0].axis('off')\n",
    "\n",
    "axes[1,1].imshow(affine_transformed_rgb)\n",
    "axes[1,1].set_title('Complex Affine Transform')\n",
    "axes[1,1].axis('off')\n",
    "\n",
    "# Show matrices\n",
    "axes[1,2].text(0.1, 0.8, f'Translation:\\n{translation_matrix}', fontsize=8)\n",
    "axes[1,2].text(0.1, 0.5, f'Affine:\\n{affine_matrix}', fontsize=8)\n",
    "axes[1,2].set_title('Transform Matrices')\n",
    "axes[1,2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Affine Transform Tips:**\n",
    "- Use `cv2.getAffineTransform()` with 3 point pairs\n",
    "- All affine transforms use `cv2.warpAffine()`\n",
    "- Matrix format: [[scale_x, shear_x, translate_x], [shear_y, scale_y, translate_y]]\n",
    "- Useful for correcting small perspective distortions\n",
    "- Preserves parallelism and ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perspective Transforms\n",
    "\n",
    "**What are perspective transforms?**\n",
    "Perspective transforms account for 3D geometry and camera effects. Unlike affine transforms, they can change the apparent size and shape based on depth.\n",
    "\n",
    "**Key differences from affine:**\n",
    "- Can converge parallel lines (like railroad tracks)\n",
    "- Requires 4 point correspondences\n",
    "- Uses 3x3 transformation matrix\n",
    "\n",
    "**Applications:**\n",
    "- Document scanning and straightening\n",
    "- Bird's eye view transformations\n",
    "- Panorama stitching\n",
    "- Augmented reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document image for perspective correction\n",
    "doc_img = cv2.imread('datasets/sample_images/doc_photo.png')\n",
    "if doc_img is not None:\n",
    "    doc_rgb = cv2.cvtColor(doc_img, cv2.COLOR_BGR2RGB)\n",
    "    print(f\"Document image shape: {doc_img.shape}\")\n",
    "else:\n",
    "    print(\"Document image not found, using sample image\")\n",
    "    doc_img = img.copy()\n",
    "    doc_rgb = img_rgb.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic perspective correction\n",
    "# Define 4 points in source image (document corners)\n",
    "height, width = doc_img.shape[:2]\n",
    "pts1 = np.float32([[30, 30], [width-30, 50], [20, height-30], [width-20, height-20]])\n",
    "\n",
    "# Define where these points should map to (rectangle)\n",
    "pts2 = np.float32([[0, 0], [400, 0], [0, 600], [400, 600]])\n",
    "\n",
    "# Get perspective transform matrix\n",
    "perspective_matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "\n",
    "# Apply transform\n",
    "perspective_corrected = cv2.warpPerspective(doc_img, perspective_matrix, (400, 600))\n",
    "perspective_corrected_rgb = cv2.cvtColor(perspective_corrected, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f\"Perspective matrix:\\n{perspective_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Creating perspective effect (opposite of correction)\n",
    "# Define rectangle points\n",
    "pts1_rect = np.float32([[0, 0], [300, 0], [0, 400], [300, 400]])\n",
    "\n",
    "# Define trapezoid points (creating perspective)\n",
    "pts2_trapezoid = np.float32([[50, 100], [250, 50], [30, 350], [280, 380]])\n",
    "\n",
    "# Get transform matrix\n",
    "perspective_effect_matrix = cv2.getPerspectiveTransform(pts1_rect, pts2_trapezoid)\n",
    "\n",
    "# Apply transform\n",
    "perspective_effect = cv2.warpPerspective(img, perspective_effect_matrix, (width, height))\n",
    "perspective_effect_rgb = cv2.cvtColor(perspective_effect, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Bird's eye view transformation\n",
    "# Simulate transforming a road view to top-down\n",
    "# This would typically require camera calibration\n",
    "road_img = cv2.imread('datasets/sample_images/road.png')\n",
    "if road_img is not None:\n",
    "    road_rgb = cv2.cvtColor(road_img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = road_img.shape[:2]\n",
    "    \n",
    "    # Define source points (trapezoid representing road)\n",
    "    src_points = np.float32([[w*0.4, h*0.6], [w*0.6, h*0.6], [w*0.1, h*0.9], [w*0.9, h*0.9]])\n",
    "    \n",
    "    # Define destination points (rectangle for bird's eye)\n",
    "    dst_points = np.float32([[0, 0], [w, 0], [0, h], [w, h]])\n",
    "    \n",
    "    # Get perspective matrix\n",
    "    birds_eye_matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "    \n",
    "    # Apply transform\n",
    "    birds_eye = cv2.warpPerspective(road_img, birds_eye_matrix, (w, h))\n",
    "    birds_eye_rgb = cv2.cvtColor(birds_eye, cv2.COLOR_BGR2RGB)\n",
    "else:\n",
    "    print(\"Road image not found\")\n",
    "    birds_eye_rgb = img_rgb.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of perspective transforms\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axes[0,0].imshow(doc_rgb)\n",
    "axes[0,0].set_title('Original Document')\n",
    "axes[0,0].axis('off')\n",
    "\n",
    "axes[0,1].imshow(perspective_corrected_rgb)\n",
    "axes[0,1].set_title('Perspective Corrected')\n",
    "axes[0,1].axis('off')\n",
    "\n",
    "axes[0,2].imshow(perspective_effect_rgb)\n",
    "axes[0,2].set_title('Perspective Effect Added')\n",
    "axes[0,2].axis('off')\n",
    "\n",
    "axes[1,0].imshow(birds_eye_rgb)\n",
    "axes[1,0].set_title('Bird\\'s Eye View')\n",
    "axes[1,0].axis('off')\n",
    "\n",
    "# Show matrices\n",
    "axes[1,1].text(0.1, 0.7, f'Correction Matrix:\\n{perspective_matrix[:2]}', fontsize=8)\n",
    "axes[1,1].text(0.1, 0.3, f'...\\n{perspective_matrix[2]}', fontsize=8)\n",
    "axes[1,1].set_title('Transform Matrix')\n",
    "axes[1,1].axis('off')\n",
    "\n",
    "# Point correspondences visualization\n",
    "axes[1,2].scatter([p[0] for p in pts1], [p[1] for p in pts1], c='red', s=50, label='Source')\n",
    "axes[1,2].scatter([p[0] for p in pts2], [p[1] for p in pts2], c='blue', s=50, label='Destination')\n",
    "for i in range(4):\n",
    "    axes[1,2].plot([pts1[i][0], pts2[i][0]], [pts1[i][1], pts2[i][1]], 'k--', alpha=0.5)\n",
    "axes[1,2].set_title('Point Correspondences')\n",
    "axes[1,2].legend()\n",
    "axes[1,2].axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perspective Transform Tips:**\n",
    "- Requires exactly 4 point pairs\n",
    "- Use `cv2.getPerspectiveTransform()` and `cv2.warpPerspective()`\n",
    "- Matrix is 3x3 (vs 2x3 for affine)\n",
    "- Can handle converging lines and depth perception\n",
    "- Point order matters (usually clockwise or counter-clockwise)\n",
    "- Useful for document scanning, AR, and 3D scene understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Image Pyramids\n",
    "\n",
    "**What are image pyramids?**\n",
    "Image pyramids are multi-resolution representations of an image. They create a series of images at different scales, useful for tasks requiring analysis at multiple levels.\n",
    "\n",
    "**Types:**\n",
    "- **Gaussian Pyramid**: Successive blurring and subsampling\n",
    "- **Laplacian Pyramid**: Used for image reconstruction and blending\n",
    "\n",
    "**Applications:**\n",
    "- Multi-scale feature detection\n",
    "- Image blending and stitching\n",
    "- Texture analysis\n",
    "- Object detection at different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Gaussian Pyramid (downsampling)\n",
    "def gaussian_pyramid(img, levels=4):\n",
    "    \"\"\"Create Gaussian pyramid by successive blurring and subsampling\"\"\"\n",
    "    pyramid = [img]\n",
    "    current = img.copy()\n",
    "    \n",
    "    for i in range(levels):\n",
    "        # Blur the image\n",
    "        blurred = cv2.GaussianBlur(current, (5, 5), 0)\n",
    "        # Downsample (reduce size by half)\n",
    "        downsampled = cv2.resize(blurred, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR)\n",
    "        pyramid.append(downsampled)\n",
    "        current = downsampled\n",
    "    \n",
    "    return pyramid\n",
    "\n",
    "# Create Gaussian pyramid\n",
    "gauss_pyramid = gaussian_pyramid(img, levels=3)\n",
    "\n",
    "print(f\"Pyramid levels: {[p.shape for p in gauss_pyramid]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Laplacian Pyramid (for reconstruction)\n",
    "def laplacian_pyramid(gauss_pyramid):\n",
    "    \"\"\"Create Laplacian pyramid from Gaussian pyramid\"\"\"\n",
    "    levels = len(gauss_pyramid) - 1\n",
    "    laplacian = []\n",
    "    \n",
    "    for i in range(levels):\n",
    "        # Upsample current level\n",
    "        upsampled = cv2.resize(gauss_pyramid[i+1], (gauss_pyramid[i].shape[1], gauss_pyramid[i].shape[0]), \n",
    "                              interpolation=cv2.INTER_LINEAR)\n",
    "        # Subtract from previous level\n",
    "        laplacian_level = cv2.subtract(gauss_pyramid[i], upsampled)\n",
    "        laplacian.append(laplacian_level)\n",
    "    \n",
    "    # Last level is the smallest Gaussian\n",
    "    laplacian.append(gauss_pyramid[-1])\n",
    "    \n",
    "    return laplacian\n",
    "\n",
    "# Create Laplacian pyramid\n",
    "laplacian_pyr = laplacian_pyramid(gauss_pyramid)\n",
    "\n",
    "print(f\"Laplacian pyramid levels: {[lp.shape for lp in laplacian_pyr]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Image blending using pyramids\n",
    "# Load two images for blending\n",
    "img1 = cv2.imread('datasets/sample_images/sample1.png')\n",
    "img2 = cv2.imread('datasets/sample_images/sample2.png')\n",
    "\n",
    "if img1 is not None and img2 is not None:\n",
    "    # Ensure same size\n",
    "    img2_resized = cv2.resize(img2, (img1.shape[1], img1.shape[0]))\n",
    "    \n",
    "    # Create masks for blending (simple horizontal split)\n",
    "    mask1 = np.zeros_like(img1, dtype=np.float32)\n",
    "    mask1[:, :img1.shape[1]//2] = 1  # Left half\n",
    "    \n",
    "    mask2 = np.zeros_like(img1, dtype=np.float32)\n",
    "    mask2[:, img1.shape[1]//2:] = 1  # Right half\n",
    "    \n",
    "    # Simple blending (could be improved with pyramid blending)\n",
    "    blended = cv2.addWeighted(img1.astype(np.float32), 0.5, img2_resized.astype(np.float32), 0.5, 0)\n",
    "    blended_rgb = cv2.cvtColor(blended.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "else:\n",
    "    blended_rgb = img_rgb.copy()\n",
    "    print(\"Images for blending not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of pyramids\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "# Gaussian Pyramid\n",
    "for i, level in enumerate(gauss_pyramid):\n",
    "    level_rgb = cv2.cvtColor(level, cv2.COLOR_BGR2RGB)\n",
    "    axes[0, i].imshow(level_rgb)\n",
    "    axes[0, i].set_title(f'Gaussian L{i}')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Laplacian Pyramid\n",
    "for i, level in enumerate(laplacian_pyr):\n",
    "    # Convert to displayable format (add 128 to center around 0)\n",
    "    level_display = cv2.cvtColor((level + 128).astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "    axes[1, i].imshow(level_display)\n",
    "    axes[1, i].set_title(f'Laplacian L{i}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# Original images and blend\n",
    "axes[2, 0].imshow(img_rgb)\n",
    "axes[2, 0].set_title('Image 1')\n",
    "axes[2, 0].axis('off')\n",
    "\n",
    "if 'img2_resized' in locals():\n",
    "    axes[2, 1].imshow(cv2.cvtColor(img2_resized, cv2.COLOR_BGR2RGB))\n",
    "    axes[2, 1].set_title('Image 2')\n",
    "    axes[2, 1].axis('off')\n",
    "\n",
    "axes[2, 2].imshow(blended_rgb)\n",
    "axes[2, 2].set_title('Blended')\n",
    "axes[2, 2].axis('off')\n",
    "\n",
    "# Pyramid info\n",
    "axes[2, 3].text(0.1, 0.5, f'Gaussian Pyramid:\\n{len(gauss_pyramid)} levels\\nSizes: {[p.shape[:2] for p in gauss_pyramid]}', \n",
    "               transform=axes[2,3].transAxes, fontsize=10, verticalalignment='center')\n",
    "axes[2, 3].set_title('Pyramid Info')\n",
    "axes[2, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Pyramids Tips:**\n",
    "- Useful for multi-scale analysis\n",
    "- Gaussian pyramid: blur + downsample\n",
    "- Laplacian pyramid: difference between levels\n",
    "- Essential for seamless image blending\n",
    "- Used in many computer vision algorithms\n",
    "- Consider memory usage with large images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "We've covered the major geometric transformations in image processing:\n",
    "\n",
    "1. **Resizing**: Change image dimensions with proper interpolation\n",
    "2. **Rotation**: Turn images around center or arbitrary points\n",
    "3. **Affine Transforms**: Preserve parallelism (translation, rotation, scaling, shearing)\n",
    "4. **Perspective Transforms**: Handle 3D geometry and converging lines\n",
    "5. **Image Pyramids**: Multi-resolution representations\n",
    "\n",
    "**General Tips:**\n",
    "- Always consider interpolation quality\n",
    "- Check coordinate systems (OpenCV vs. mathematical)\n",
    "- Handle boundary conditions properly\n",
    "- Consider performance vs. quality trade-offs\n",
    "- Test transforms with known inputs first\n",
    "\n",
    "**Performance Considerations:**\n",
    "- Perspective transforms are most computationally expensive\n",
    "- Use appropriate data types\n",
    "- Consider GPU acceleration for real-time applications\n",
    "\n",
    "Practice these transforms with your own images and explore combining multiple transformations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
