{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11. Text Summarization\n",
        "Text summarization is a crucial task in Natural Language Processing (NLP) that involves condensing large volumes of text into shorter, coherent summaries while retaining the essential information. This technique is widely used in various applications, such as news aggregation, document summarization, and information retrieval, to help users quickly grasp the main points of lengthy documents.\n",
        "\n",
        "### What You'll Learn:\n",
        "- Extractive vs Abstractive summarization\n",
        "- Algorithms\n",
        "- Evaluation metrics\n",
        "- Practical implementation\n",
        "- Real-world applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two Types of Summarization\n",
        "\n",
        "### Extractive Summarization\n",
        "- Select important sentences from original text\n",
        "- Faster\n",
        "- Preserves original language\n",
        "- Use case: News articles\n",
        "\n",
        "### Abstractive Summarization\n",
        "- Generate new sentences\n",
        "- Slower but more natural\n",
        "- Requires language generation\n",
        "- Use case: Professional reports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extractive Summarization Methods\n",
        "\n",
        "1. **TF-IDF based**: Select sentences with highest word importance\n",
        "2. **TextRank**: Treat sentences as graph, find most important\n",
        "3. **Frequency-based**: Select sentences with most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EXTRACTIVE SUMMARIZATION WITH TF-IDF\n",
            "============================================================\n",
            "\n",
            "Original text:\n",
            "Machine learning is a subset of artificial intelligence. \n",
            "It enables computers to learn from data without explicit programming. \n",
            "Deep learning is a subset of machine learning. \n",
            "It uses neural networks with multiple layers. \n",
            "Natural language processing is an important field. \n",
            "It focuses on interaction between computers and human language.\n",
            "\n",
            "Total sentences: 6\n",
            "\n",
            "Summary (top 2 sentences):\n",
            "  - Machine learning is a subset of artificial intelligence.\n",
            "  - \n",
            "It enables computers to learn from data without explicit programming.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "print('='*60)\n",
        "print('EXTRACTIVE SUMMARIZATION WITH TF-IDF')\n",
        "print('='*60)\n",
        "\n",
        "text = '''Machine learning is a subset of artificial intelligence. \n",
        "It enables computers to learn from data without explicit programming. \n",
        "Deep learning is a subset of machine learning. \n",
        "It uses neural networks with multiple layers. \n",
        "Natural language processing is an important field. \n",
        "It focuses on interaction between computers and human language.'''\n",
        "\n",
        "sentences = text.split('. ')\n",
        "print('\\nOriginal text:')\n",
        "print(text)\n",
        "print(f'\\nTotal sentences: {len(sentences)}\\n')\n",
        "\n",
        "# Extract features\n",
        "vectorizer = TfidfVectorizer(max_features=10, stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Get sentence scores\n",
        "sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
        "\n",
        "# Get top 2 sentences\n",
        "top_indices = sentence_scores.argsort()[-2:][::-1]\n",
        "top_sentences = [sentences[i] for i in sorted(top_indices)]\n",
        "\n",
        "print('Summary (top 2 sentences):')\n",
        "for sent in top_sentences:\n",
        "    print(f'  - {sent}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Abstractive Summarization with Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ABSTRACTIVE SUMMARIZATION WITH TRANSFORMERS\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Requires transformers library and internet connection)\n",
            "Process:\n",
            "1. Load pre-trained summarization model\n",
            "2. Tokenize input text\n",
            "3. Generate summary tokens\n",
            "4. Decode to get summary\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "print('\\nABSTRACTIVE SUMMARIZATION WITH TRANSFORMERS')\n",
        "\n",
        "try:\n",
        "    summarizer = pipeline('summarization', model='facebook/bart-large-cnn')\n",
        "    \n",
        "    text = 'Machine learning enables computers to learn from data. ' \\\n",
        "           'Deep learning uses neural networks. ' \\\n",
        "           'Transformers revolutionized NLP. ' \\\n",
        "           'BERT is a famous transformer model. ' \\\n",
        "           'These models achieve state-of-the-art results.'\n",
        "    \n",
        "    # Summarize\n",
        "    summary = summarizer(text, max_length=50, min_length=20, do_sample=False)\n",
        "    print(f'\\nOriginal: {text}')\n",
        "    print(f'\\nSummary: {summary[0][\"summary_text\"]}')\n",
        "    \n",
        "except Exception as e:\n",
        "    print('(Requires transformers library and internet connection)')\n",
        "    print('Process:')\n",
        "    print('1. Load pre-trained summarization model')\n",
        "    print('2. Tokenize input text')\n",
        "    print('3. Generate summary tokens')\n",
        "    print('4. Decode to get summary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "### ROUGE Score\n",
        "Compares generated summary with reference summary\n",
        "- ROUGE-1: Unigram overlap\n",
        "- ROUGE-2: Bigram overlap\n",
        "- ROUGE-L: Longest common sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EVALUATION EXAMPLE:\n",
            "Reference: machine learning is powerful\n",
            "Generated: machine learning is great\n",
            "\n",
            "Precision: 75.00%\n",
            "Recall: 75.00%\n"
          ]
        }
      ],
      "source": [
        "print('\\nEVALUATION EXAMPLE:')\n",
        "from collections import Counter\n",
        "\n",
        "reference = 'machine learning is powerful'\n",
        "generated = 'machine learning is great'\n",
        "\n",
        "ref_words = reference.split()\n",
        "gen_words = generated.split()\n",
        "\n",
        "# Simple precision/recall\n",
        "matching = len(set(ref_words) & set(gen_words))\n",
        "precision = matching / len(gen_words)\n",
        "recall = matching / len(ref_words)\n",
        "\n",
        "print(f'Reference: {reference}')\n",
        "print(f'Generated: {generated}')\n",
        "print(f'\\nPrecision: {precision:.2%}')\n",
        "print(f'Recall: {recall:.2%}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
