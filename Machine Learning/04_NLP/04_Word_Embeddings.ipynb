{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04. Word Embeddings: Word2Vec, GloVe, FastText",
        "",
        "## Course Level: Intermediate (\u2b50\u2b50)",
        "",
        "### What You'll Learn:",
        "- Why embeddings are better than BoW/TF-IDF",
        "- Word2Vec (Skip-gram, CBOW)",
        "- GloVe",
        "- FastText",
        "- Finding similar words",
        "- Word relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Word Embeddings?",
        "",
        "**Problem with BoW/TF-IDF**:",
        "- Sparse vectors (mostly zeros)",
        "- No semantic meaning",
        "- 'king' and 'queen' have no relationship",
        "",
        "**Solution: Dense Embeddings**:",
        "- Dense vectors (small fixed size)",
        "- Captures semantic meaning",
        "- Related words have similar vectors",
        "- Can do math: king - man + woman \u2248 queen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2Vec",
        "",
        "### Concept:",
        "- Creates dense word vectors (typically 100-300 dimensions)",
        "- Captures semantic relationships",
        "- Two architectures:",
        "  - **CBOW**: Predict word from context",
        "  - **Skip-gram**: Predict context from word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec",
        "import numpy as np",
        "",
        "sentences = [",
        "    'machine learning is awesome',",
        "    'deep learning with neural networks',",
        "    'python for machine learning',",
        "    'natural language processing is powerful'",
        "]",
        "",
        "# Create word tokens",
        "tokenized = [s.split() for s in sentences]",
        "",
        "print('='*60)",
        "print('WORD2VEC EXAMPLE')",
        "print('='*60)",
        "print('Training sentences:', tokenized)",
        "",
        "# Train Word2Vec model",
        "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=1, workers=4)",
        "print('\\nModel trained!')",
        "print(f'Vocabulary size: {len(model.wv)}')",
        "",
        "# Get word vector",
        "print('\\nWord vector for \"learning\":')",
        "print(model.wv['learning'][:10], '...')  # Show first 10 dimensions",
        "print(f'Vector shape: {model.wv[\"learning\"].shape}')",
        "",
        "# Find similar words",
        "print('\\nWords similar to \"learning\":')",
        "similar = model.wv.most_similar('learning', topn=3)",
        "for word, score in similar:",
        "    print(f'  {word}: {score:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word Arithmetic",
        "",
        "Word2Vec captures relationships:",
        "- king - man + woman \u2248 queen",
        "- france - paris + london \u2248 england"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word arithmetic",
        "print('\\nWORD ARITHMETIC EXAMPLE:')",
        "try:",
        "    result = model.wv.most_similar(positive=['learning', 'python'], topn=3)",
        "    print('Words related to (learning + python):')",
        "    for word, score in result:",
        "        print(f'  {word}: {score:.3f}')",
        "except:",
        "    print('Need more training data for arithmetic operations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GloVe vs Word2Vec",
        "",
        "| Feature | Word2Vec | GloVe |",
        "|---------|----------|-------|",
        "| Speed | Fast | Slower |",
        "| Quality | Good | Better |",
        "| Training | Neural Network | Matrix factorization |",
        "| Best for | Large corpora | Balanced |",
        "",
        "(Pre-trained models recommended for GloVe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FastText",
        "",
        "**Advantages over Word2Vec**:",
        "- Handles out-of-vocabulary words",
        "- Uses character n-grams",
        "- Better for morphologically rich languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import FastText",
        "",
        "# Train FastText model",
        "ft_model = FastText(tokenized, vector_size=100, window=5, workers=4)",
        "",
        "print('\\nFASTTEXT EXAMPLE')",
        "print(f'Vocabulary size: {len(ft_model.wv)}')",
        "",
        "# FastText can handle unknown words",
        "print('\\nVector for \"learning\":', ft_model.wv['learning'][:5], '...')",
        "",
        "# Can get vectors for words not in training data",
        "try:",
        "    print('Vector for \"xyz_unknown_word\":', ft_model.wv['xyz_unknown_word'][:5], '...')",
        "except:",
        "    print('(Would work with more training data)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}