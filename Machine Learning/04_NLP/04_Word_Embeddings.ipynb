{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04. Word Embeddings: Word2Vec, GloVe, FastText\n",
        "\n",
        "### What You'll Learn:\n",
        "- Why embeddings are better than BoW/TF-IDF\n",
        "- Word2Vec (Skip-gram, CBOW)\n",
        "- GloVe\n",
        "- FastText\n",
        "- Finding similar words\n",
        "- Word relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Word Embeddings?\n",
        "\n",
        "**Problem with BoW/TF-IDF**:\n",
        "- Sparse vectors (mostly zeros)\n",
        "- No semantic meaning\n",
        "- 'king' and 'queen' have no relationship\n",
        "\n",
        "**Solution: Dense Embeddings**:\n",
        "- Dense vectors (small fixed size)\n",
        "- Captures semantic meaning\n",
        "- Related words have similar vectors\n",
        "- Can do math: king - man + woman ≈ queen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2Vec\n",
        "\n",
        "### Concept:\n",
        "- Creates dense word vectors (typically 100-300 dimensions)\n",
        "- Captures semantic relationships\n",
        "- Two architectures:\n",
        "  - **CBOW**: Predict word from context\n",
        "  - **Skip-gram**: Predict context from word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "WORD2VEC EXAMPLE\n",
            "============================================================\n",
            "Training sentences: [['machine', 'learning', 'is', 'awesome'], ['deep', 'learning', 'with', 'neural', 'networks'], ['python', 'for', 'machine', 'learning'], ['natural', 'language', 'processing', 'is', 'powerful']]\n",
            "\n",
            "Model trained!\n",
            "Vocabulary size: 14\n",
            "\n",
            "Word vector for \"learning\":\n",
            "[-0.00053623  0.00023643  0.00510335  0.00900927 -0.00930295 -0.00711681\n",
            "  0.00645887  0.00897299 -0.00501543 -0.00376337] ...\n",
            "Vector shape: (100,)\n",
            "\n",
            "Words similar to \"learning\":\n",
            "  networks: 0.216\n",
            "  deep: 0.093\n",
            "  for: 0.093\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "sentences = [\n",
        "    'machine learning is awesome',\n",
        "    'deep learning with neural networks',\n",
        "    'python for machine learning',\n",
        "    'natural language processing is powerful'\n",
        "]\n",
        "\n",
        "# Create word tokens\n",
        "tokenized = [s.split() for s in sentences]\n",
        "\n",
        "print('='*60)\n",
        "print('WORD2VEC EXAMPLE')\n",
        "print('='*60)\n",
        "print('Training sentences:', tokenized)\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=1, workers=4)\n",
        "print('\\nModel trained!')\n",
        "print(f'Vocabulary size: {len(model.wv)}')\n",
        "\n",
        "# Get word vector\n",
        "print('\\nWord vector for \"learning\":')\n",
        "print(model.wv['learning'][:10], '...')  # Show first 10 dimensions\n",
        "print(f'Vector shape: {model.wv[\"learning\"].shape}')\n",
        "\n",
        "# Find similar words\n",
        "print('\\nWords similar to \"learning\":')\n",
        "similar = model.wv.most_similar('learning', topn=3)\n",
        "for word, score in similar:\n",
        "    print(f'  {word}: {score:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word Arithmetic\n",
        "\n",
        "Word2Vec captures relationships:\n",
        "- king - man + woman ≈ queen\n",
        "- france - paris + london ≈ england"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WORD ARITHMETIC EXAMPLE:\n",
            "Words related to (learning + python):\n",
            "  networks: 0.182\n",
            "  machine: 0.102\n",
            "  with: 0.082\n"
          ]
        }
      ],
      "source": [
        "# Word arithmetic\n",
        "print('\\nWORD ARITHMETIC EXAMPLE:')\n",
        "try:\n",
        "    result = model.wv.most_similar(positive=['learning', 'python'], topn=3)\n",
        "    print('Words related to (learning + python):')\n",
        "    for word, score in result:\n",
        "        print(f'  {word}: {score:.3f}')\n",
        "except:\n",
        "    print('Need more training data for arithmetic operations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GloVe vs Word2Vec\n",
        "\n",
        "| Feature | Word2Vec | GloVe |\n",
        "|---------|----------|-------|\n",
        "| Speed | Fast | Slower |\n",
        "| Quality | Good | Better |\n",
        "| Training | Neural Network | Matrix factorization |\n",
        "| Best for | Large corpora | Balanced |\n",
        "\n",
        "(Pre-trained models recommended for GloVe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FastText\n",
        "\n",
        "**Advantages over Word2Vec**:\n",
        "- Handles out-of-vocabulary words\n",
        "- Uses character n-grams\n",
        "- Better for morphologically rich languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FASTTEXT EXAMPLE\n",
            "Vocabulary size: 21\n",
            "\n",
            "Vector for \"learning\": [ 1.1518626e-03 -6.4418455e-05 -1.1798259e-03  6.3712716e-05\n",
            " -1.6880927e-03] ...\n",
            "Vector for \"xyz_unknown_word\": [ 0.00037216 -0.00091789  0.00090347  0.00019503  0.00145477] ...\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Train FastText model\n",
        "#ft_model = FastText(tokenized, vector_size=100, window=5, workers=4)\n",
        "ft_model = FastText(\n",
        "    sentences=sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1\n",
        ")\n",
        "\n",
        "print('\\nFASTTEXT EXAMPLE')\n",
        "print(f'Vocabulary size: {len(ft_model.wv)}')\n",
        "\n",
        "# FastText can handle unknown words\n",
        "print('\\nVector for \"learning\":', ft_model.wv['learning'][:5], '...')\n",
        "\n",
        "# Can get vectors for words not in training data\n",
        "try:\n",
        "    print('Vector for \"xyz_unknown_word\":', ft_model.wv['xyz_unknown_word'][:5], '...')\n",
        "except:\n",
        "    print('(Would work with more training data)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
