{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 09. Sequence Models: RNN, LSTM, GRU",
        "",
        "## Course Level: Advanced (\u2b50\u2b50\u2b50)",
        "",
        "### What You'll Learn:",
        "- Why sequence models for NLP",
        "- RNN limitations",
        "- LSTM architecture",
        "- GRU architecture",
        "- Practical implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Sequence Models?",
        "",
        "**Problems with traditional ML**:",
        "- No memory of previous inputs",
        "- Can't handle variable length sequences",
        "- Lost temporal relationships",
        "",
        "**Solution: Recurrent Neural Networks (RNNs)**",
        "- Have memory/state",
        "- Process sequences",
        "- Perfect for NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RNN Problem: Vanishing Gradient",
        "",
        "- In deep RNNs, gradients become very small",
        "- Model can't learn long-term dependencies",
        "- Solution: LSTM and GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM (Long Short-Term Memory)",
        "",
        "**Key Components**:",
        "- **Input Gate**: What new info to store",
        "- **Forget Gate**: What old info to forget",
        "- **Output Gate**: What to output",
        "- **Cell State**: Long-term memory",
        "",
        "Advantage: Can learn dependencies far apart in sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout",
        "from tensorflow.keras.models import Sequential",
        "",
        "print('='*60)",
        "print('LSTM MODEL FOR SENTIMENT CLASSIFICATION')",
        "print('='*60)",
        "",
        "# Create LSTM model",
        "model = Sequential([",
        "    Embedding(1000, 64, input_length=100),  # Embedding layer",
        "    LSTM(128, return_sequences=True),        # LSTM layer",
        "    Dropout(0.2),                             # Dropout for regularization",
        "    LSTM(64),                                 # Another LSTM layer",
        "    Dense(32, activation='relu'),             # Dense layer",
        "    Dense(1, activation='sigmoid')            # Output layer",
        "])",
        "",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])",
        "",
        "print('\\nModel Architecture:')",
        "model.summary()",
        "",
        "print('\\n\u2713 LSTM model created')",
        "print('\\nModel Explanation:')",
        "print('- Embedding: Converts word IDs to vectors')",
        "print('- LSTM layer 1: Learns long-term dependencies')",
        "print('- LSTM layer 2: Refines learned patterns')",
        "print('- Dense layers: Classification')",
        "print('- Output: Probability (0=negative, 1=positive)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRU (Gated Recurrent Unit)",
        "",
        "**Similar to LSTM but**:",
        "- Simpler architecture (fewer parameters)",
        "- Faster training",
        "- Often similar performance",
        "- Good alternative to LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import GRU",
        "",
        "# Create GRU model",
        "model_gru = Sequential([",
        "    Embedding(1000, 64, input_length=100),",
        "    GRU(128, return_sequences=True),",
        "    Dropout(0.2),",
        "    GRU(64),",
        "    Dense(32, activation='relu'),",
        "    Dense(1, activation='sigmoid')",
        "])",
        "",
        "model_gru.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])",
        "",
        "print('GRU Model Created')",
        "print('\\nComparison:')",
        "print('LSTM: More parameters, slower, better for complex patterns')",
        "print('GRU: Fewer parameters, faster, simpler alternative')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}