{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Transformers & BERT\n",
        "Transformers and BERT (Bidirectional Encoder Representations from Transformers) have revolutionized the field of Natural Language Processing (NLP) by enabling models to understand context and relationships in text more effectively than previous architectures. Transformers utilize self-attention mechanisms to process input data in parallel, allowing for better handling of long-range dependencies in text. BERT, built on the Transformer architecture, is pre-trained on large corpora and can be fine-tuned for various NLP tasks, achieving state-of-the-art performance.\n",
        "\n",
        "### What You'll Learn:\n",
        "- Transformer architecture\n",
        "- Attention mechanism\n",
        "- BERT explained\n",
        "- Fine-tuning BERT\n",
        "- Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem with RNN/LSTM\n",
        "\n",
        "- Process sequentially (slow)\n",
        "- Hard to parallelize\n",
        "- Still has gradient issues\n",
        "\n",
        "**Solution: Transformers**\n",
        "- Process entire sequence at once (parallel)\n",
        "- Attention mechanism for relationships\n",
        "- State-of-the-art performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention Mechanism\n",
        "\n",
        "**Core Idea**: Focus on relevant parts of input\n",
        "\n",
        "**Query, Key, Value**:\n",
        "- Query: What am I looking for?\n",
        "- Key: What does this contain?\n",
        "- Value: What information is here?\n",
        "\n",
        "**Result**: Context-aware representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT (Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "**Why BERT is powerful**:\n",
        "- **Bidirectional**: Reads left-to-right AND right-to-left\n",
        "- **Pre-trained**: Already knows English\n",
        "- **Transfer Learning**: Fine-tune for your task\n",
        "- **Contextual**: Same word has different meaning based on context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "BERT FOR SENTIMENT CLASSIFICATION\n",
            "============================================================\n",
            "(Transformers example - requires internet for first download)\n",
            "Error: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "print('='*60)\n",
        "print('BERT FOR SENTIMENT CLASSIFICATION')\n",
        "print('='*60)\n",
        "\n",
        "try:\n",
        "    # Using pre-trained sentiment classifier\n",
        "    classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "    \n",
        "    texts = [\n",
        "        'I absolutely love this product!',\n",
        "        'This is terrible',\n",
        "        'It is ok'\n",
        "    ]\n",
        "    \n",
        "    print('\\nBERT Predictions:')\n",
        "    for text in texts:\n",
        "        result = classifier(text)[0]\n",
        "        print(f'  \"{text}\"')\n",
        "        print(f'    -> {result[\"label\"]}: {result[\"score\"]:.2%}\\n')\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f'(Transformers example - requires internet for first download)')\n",
        "    print(f'Error: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tuning BERT\n",
        "\n",
        "Adapt pre-trained BERT for your specific task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BERT FINE-TUNING PROCESS:\n",
            "1. Load pre-trained BERT model\n",
            "2. Add task-specific layers\n",
            "3. Train on your labeled data\n",
            "4. Evaluate on test set\n",
            "\n",
            "Advantages:\n",
            "- Faster training (transfer learning)\n",
            "- Need less data\n",
            "- Better results than training from scratch\n",
            "- Can adapt to specific domains\n"
          ]
        }
      ],
      "source": [
        "print('\\nBERT FINE-TUNING PROCESS:')\n",
        "print('1. Load pre-trained BERT model')\n",
        "print('2. Add task-specific layers')\n",
        "print('3. Train on your labeled data')\n",
        "print('4. Evaluate on test set')\n",
        "print('\\nAdvantages:')\n",
        "print('- Faster training (transfer learning)')\n",
        "print('- Need less data')\n",
        "print('- Better results than training from scratch')\n",
        "print('- Can adapt to specific domains')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
