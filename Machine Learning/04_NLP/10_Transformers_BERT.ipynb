{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Transformers & BERT",
        "",
        "## Course Level: Advanced (\u2b50\u2b50\u2b50)",
        "",
        "### What You'll Learn:",
        "- Transformer architecture",
        "- Attention mechanism",
        "- BERT explained",
        "- Fine-tuning BERT",
        "- Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem with RNN/LSTM",
        "",
        "- Process sequentially (slow)",
        "- Hard to parallelize",
        "- Still has gradient issues",
        "",
        "**Solution: Transformers**",
        "- Process entire sequence at once (parallel)",
        "- Attention mechanism for relationships",
        "- State-of-the-art performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention Mechanism",
        "",
        "**Core Idea**: Focus on relevant parts of input",
        "",
        "**Query, Key, Value**:",
        "- Query: What am I looking for?",
        "- Key: What does this contain?",
        "- Value: What information is here?",
        "",
        "**Result**: Context-aware representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT (Bidirectional Encoder Representations from Transformers)",
        "",
        "**Why BERT is powerful**:",
        "- **Bidirectional**: Reads left-to-right AND right-to-left",
        "- **Pre-trained**: Already knows English",
        "- **Transfer Learning**: Fine-tune for your task",
        "- **Contextual**: Same word has different meaning based on context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification",
        "import torch",
        "",
        "print('='*60)",
        "print('BERT FOR SENTIMENT CLASSIFICATION')",
        "print('='*60)",
        "",
        "try:",
        "    # Using pre-trained sentiment classifier",
        "    classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')",
        "    ",
        "    texts = [",
        "        'I absolutely love this product!',",
        "        'This is terrible',",
        "        'It is ok'",
        "    ]",
        "    ",
        "    print('\\nBERT Predictions:')",
        "    for text in texts:",
        "        result = classifier(text)[0]",
        "        print(f'  \"{text}\"')",
        "        print(f'    -> {result[\"label\"]}: {result[\"score\"]:.2%}\\n')",
        "        ",
        "except Exception as e:",
        "    print(f'(Transformers example - requires internet for first download)')",
        "    print(f'Error: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tuning BERT",
        "",
        "Adapt pre-trained BERT for your specific task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nBERT FINE-TUNING PROCESS:')",
        "print('1. Load pre-trained BERT model')",
        "print('2. Add task-specific layers')",
        "print('3. Train on your labeled data')",
        "print('4. Evaluate on test set')",
        "print('\\nAdvantages:')",
        "print('- Faster training (transfer learning)')",
        "print('- Need less data')",
        "print('- Better results than training from scratch')",
        "print('- Can adapt to specific domains')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}