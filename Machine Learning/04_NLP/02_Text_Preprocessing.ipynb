{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02. Text Preprocessing & Data Cleaning\n",
        "Text preprocessing and data cleaning are essential steps in Natural Language Processing (NLP) that involve preparing and transforming raw text data into a structured format suitable for analysis and modeling.\n",
        "\n",
        "### What You'll Learn:\n",
        "- Why text preprocessing matters\n",
        "- Text cleaning techniques\n",
        "- Tokenization methods\n",
        "- Normalization strategies\n",
        "- Stemming vs Lemmatization\n",
        "- Real-world examples and best practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why is Text Preprocessing Important?\n",
        "\n",
        "Raw text data is messy and contains:\n",
        "- **Noise**: Special characters, URLs, HTML tags\n",
        "- **Inconsistency**: Mixed case, extra spaces\n",
        "- **Irrelevant data**: Stopwords, punctuation\n",
        "- **Variations**: running, runs, ran (same meaning)\n",
        "\n",
        "**Without preprocessing**: Poor accuracy\n",
        "**With preprocessing**: Better model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Libraries loaded\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download required data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "print('✓ Libraries loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Remove Special Characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: Hello! This is TEST... @#$% with 123 numbers!!!\n",
            "Cleaned: Hello This is TEST  with 123 numbers\n"
          ]
        }
      ],
      "source": [
        "raw_text = 'Hello! This is TEST... @#$% with 123 numbers!!!'\n",
        "print('Original:', raw_text)\n",
        "\n",
        "# Remove special characters\n",
        "cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', raw_text)\n",
        "print('Cleaned:', cleaned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Remove URLs and Emails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: Visit https://example.com or email support@example.com\n",
            "Cleaned: Visit  or email \n"
          ]
        }
      ],
      "source": [
        "text = 'Visit https://example.com or email support@example.com'\n",
        "print('Original:', text)\n",
        "\n",
        "# Remove URLs\n",
        "text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "text = re.sub(r'\\S+@\\S+', '', text)\n",
        "print('Cleaned:', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: Hello world! How are you?\n",
            "\n",
            "Word tokens: ['Hello', 'world', '!', 'How', 'are', 'you', '?']\n",
            "Sentence tokens: ['Hello world!', 'How are you?']\n"
          ]
        }
      ],
      "source": [
        "text = 'Hello world! How are you?'\n",
        "print('Original:', text)\n",
        "\n",
        "# Word tokenization\n",
        "words = word_tokenize(text, language=\"english\", preserve_line=False)\n",
        "print('\\nWord tokens:', words)\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(text, language=\"english\")\n",
        "print('Sentence tokens:', sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tokens: ['the', 'quick', 'brown', 'fox', 'jumps']\n",
            "Filtered tokens: ['quick', 'brown', 'fox', 'jumps']\n"
          ]
        }
      ],
      "source": [
        "text = 'the quick brown fox jumps'\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "print('Original tokens:', tokens)\n",
        "print('Filtered tokens:', filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Lemmatization vs Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word | Stemming | Lemmatization\n",
            "running | run | run\n",
            "caring | care | care\n",
            "studies | studi | study\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['running', 'caring', 'studies']\n",
        "\n",
        "print('Word | Stemming | Lemmatization')\n",
        "for word in words:\n",
        "    stem = stemmer.stem(word)\n",
        "    lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "    print(f'{word} | {stem} | {lemma}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
