{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02. Text Preprocessing & Data Cleaning",
        "",
        "## Course Level: Beginner \u2b50",
        "",
        "### What You'll Learn:",
        "- Why text preprocessing matters",
        "- Text cleaning techniques",
        "- Tokenization methods",
        "- Normalization strategies",
        "- Stemming vs Lemmatization",
        "- Real-world examples and best practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why is Text Preprocessing Important?",
        "",
        "Raw text data is messy and contains:",
        "- **Noise**: Special characters, URLs, HTML tags",
        "- **Inconsistency**: Mixed case, extra spaces",
        "- **Irrelevant data**: Stopwords, punctuation",
        "- **Variations**: running, runs, ran (same meaning)",
        "",
        "**Without preprocessing**: Poor accuracy",
        "**With preprocessing**: Better model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re",
        "import string",
        "from nltk.tokenize import word_tokenize, sent_tokenize",
        "from nltk.corpus import stopwords",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer",
        "import nltk",
        "",
        "# Download required data",
        "nltk.download('punkt', quiet=True)",
        "nltk.download('stopwords', quiet=True)",
        "nltk.download('wordnet', quiet=True)",
        "",
        "print('\u2713 Libraries loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Remove Special Characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_text = 'Hello! This is TEST... @#$% with 123 numbers!!!'",
        "print('Original:', raw_text)",
        "",
        "# Remove special characters",
        "cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', raw_text)",
        "print('Cleaned:', cleaned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Remove URLs and Emails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = 'Visit https://example.com or email support@example.com'",
        "print('Original:', text)",
        "",
        "# Remove URLs",
        "text = re.sub(r'http\\S+|www\\S+', '', text)",
        "text = re.sub(r'\\S+@\\S+', '', text)",
        "print('Cleaned:', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = 'Hello world! How are you?'",
        "print('Original:', text)",
        "",
        "# Word tokenization",
        "words = word_tokenize(text)",
        "print('\\nWord tokens:', words)",
        "",
        "# Sentence tokenization",
        "sentences = sent_tokenize(text)",
        "print('Sentence tokens:', sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = 'the quick brown fox jumps'",
        "tokens = word_tokenize(text.lower())",
        "",
        "stop_words = set(stopwords.words('english'))",
        "filtered = [w for w in tokens if w not in stop_words]",
        "",
        "print('Original tokens:', tokens)",
        "print('Filtered tokens:', filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Lemmatization vs Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer",
        "",
        "stemmer = PorterStemmer()",
        "lemmatizer = WordNetLemmatizer()",
        "",
        "words = ['running', 'caring', 'studies']",
        "",
        "print('Word | Stemming | Lemmatization')",
        "for word in words:",
        "    stem = stemmer.stem(word)",
        "    lemma = lemmatizer.lemmatize(word, pos='v')",
        "    print(f'{word} | {stem} | {lemma}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}